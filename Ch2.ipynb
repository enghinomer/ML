{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from matplotlib import style\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# import seaborn\n",
    "import seaborn as sns\n",
    "# settings for seaborn plotting style\n",
    "sns.set(color_codes=True)\n",
    "# settings for seaborn plot sizes\n",
    "sns.set(rc={'figure.figsize':(5,5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Binary variables</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bernoulli distribution</h2>\n",
    "<p>This is a distribution over a single binary variable $x \\in \\{0,1\\}$ representing for example the result of flipping a coin. Its parameter is $\\mu$ representing the probability that x=1. The conjugate prior for $\\mu$ is the Beta distribution.</p>\n",
    "\n",
    "$$Bern(x|\\mu)=\\mu^{x}(1-\\mu)^{1-x}$$\n",
    "\n",
    "$$E[x]=\\mu $$\n",
    "\n",
    "$$ var[x] = \\mu(1-\\mu) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2242c366dd144bb888abb2cd49116672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='mu', max=1.0), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def bernoulli_distribution(mu=(0.0,1.0)):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    x = np.arange(2)\n",
    "    ax.plot(x, bernoulli.pmf(x,p=mu), 'bo', ms=8, label='bernoulli pmf')\n",
    "    ax.vlines(x, 0, bernoulli.pmf(x,p=mu), colors='b', lw=5, alpha=0.5)\n",
    "    ax.set(xlabel='Bernoulli Distribution', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Binomial distribution</h2>\n",
    "<p>The binomial distribution gives the probability of observing m occurences of x=1 in a set of N samples where the probability of observing x=1 is $\\mu \\in [0,1]$. The conjugate prior for $\\mu$ is the Beta distribution.</p>\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Bin(m|N,\\mu) = \\frac{N!}{(N-m)!m!}\\mu^{m}(1-\\mu)^{N-m} \n",
    "\\label{eq:binomial_dist} \\tag{1}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$ E[m]=N\\mu $$\n",
    "\n",
    "$$ var[m]=N\\mu(1-\\mu) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bbbd824afd4fd1ab1171b814a6d9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='N', min=1), FloatSlider(value=0.5, description='mu', ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def binomial_distribution(N=(1,100), mu=(0.0,1.0, 0.05)):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    x = np.arange(N+1)\n",
    "    ax.xaxis.set_ticks(np.arange(0, N+2, int(round(N/11))))\n",
    "    ax.plot(x, binom.pmf(x, N, mu), 'bo', ms=8, label='binom pmf')\n",
    "    ax.vlines(x, 0, binom.pmf(x, N, mu), colors='b', lw=5, alpha=0.5)\n",
    "    ax.set(xlabel='Binomial Distribution', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Beta distribution</h1>\n",
    "\n",
    "<p>The conjugate prior for the Bernoulli and Binomial distribution is the Beta distribution</p>\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Beta(\\mu|a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\mu^{a-1}(1-\\mu)^{b-1} \n",
    "\\label{eq:beta_dist} \\tag{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "E[\\mu] = \\frac{a}{a+b} \n",
    "\\label{eq:beta_expectation} \\tag{3}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$ var[\\mu] = \\frac{ab}{(a+b)^2(a+b+1)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e979098107446119efefcde10b52283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=500.0, description='a', max=1000.0), FloatSlider(value=500.0, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def beta_distribution(a=(0.0,1000), b=(0.0,1000)):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    x = np.linspace(0,1,100)\n",
    "    #ax.xaxis.set_ticks(np.arange(0, n, int(round(n/10))))\n",
    "    ax.plot(x, beta.pdf(x, a, b), 'r-', lw=5, alpha=0.6, label='beta pdf')\n",
    "    ax.set(xlabel='Beta Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's suppose we have a data set $ D= {x_1, x_2, ... , x_N}$ where $x_i$ comes from a Bernoulli distribution and we want to estimate $\\mu$. In a <b>frequentist</b> setting we can estimate $\\mu$ by maximizing the <b>maximum likelihood</b>:</p>\n",
    "\n",
    "$$ p(D|\\mu) = \\prod_{n=1}^N p(x_n|\\mu) = \\prod_{n=1}^N \\mu^{x_n}(1-\\mu)^{1-x_n}$$\n",
    "\n",
    "or the log likelihood:\n",
    "\n",
    "$$ \\ln p(D|\\mu) = \\sum_{n=1}^N \\ln p(x_n|\\mu) = \\sum_{n=1}^N [x_n\\ln \\mu + (1-x_n)\\ln(1-\\mu)]$$\n",
    "\n",
    "This gives us\n",
    "\n",
    "$$ \\mu_{ML} = \\frac{1}{N} \\sum_{n=1}^N x_n = \\frac{m}{N}$$ where m is the number of observations where x = 1. The above problem can be stated using the Binomial distribution.\n",
    "\n",
    "<p>Consider our data set D has only one observation $x_1 = 1$. This will give us m=N=1 and $\\mu_ML = 1$. This means that all future observations will be 1 which obviously is unreasonable.</p>\n",
    "\n",
    "<p>To overcome this problem a <b>Bayesian</b> approachmight be helpful. For this we need a prior distribution $p(\\mu)$. The conjugate prior for the Binomial distribution is the Beta distribution (2).</p>\n",
    "\n",
    "<p>The posterior distribution of $\\mu$ is obtained by multiplying the beta prior (2) with binomial likelihood function (1) and we get after normalization:</p>\n",
    "\n",
    "$$ p(\\mu|m,l,a,b)= \\frac{\\Gamma(m+a+l+b)}{\\Gamma(m+a)\\Gamma(l+b)}\\mu^{m+a-1}(1-\\mu)^{l+b-1} $$\n",
    "\n",
    "We can see that a and b correspond to the number of observations of x=1 and x=0.\n",
    "\n",
    "To predict the outcome of the next trial we compute the predictive <b>posterior predictive distribution</b> of x given the observed data D:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x=1|D) &= \\int_{0}^1 p(x=1,\\mu|D)d\\mu \\\\\n",
    "         &= \\int_{0}^1 p(x=1|\\mu,D)p(\\mu|D)d\\mu \\\\\n",
    "         &= \\int_{0}^1 \\mu p(\\mu|D) \\\\\n",
    "         &= E[\\mu|D] \\\\\n",
    "         &= \\frac{m+a}{m+a+l+b}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multinomial variables</h1>\n",
    "\n",
    "<h2>Multinomial distribution</h2>\n",
    "Represents variables that can take one of K states. Ex:\n",
    "\n",
    "$$ x=(0,0,1,0,0,0)^T $$ here K=6 and $x_3=1$. Wesay $\\mu_k$ is the probability that $x_k=1$. Hence the distribution of x is given by:\n",
    "\n",
    "$$ p(x|\\mu)= \\prod_{k=1}^K \\mu_{k}^{x_k} $$ where \n",
    "\n",
    "$$ \\mu = (\\mu_1... \\mu_K)^T $$\n",
    "\n",
    "We have \n",
    "\n",
    "$$ E[x|\\mu] = \\mu $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's denote D a set of N independent observations ${x_1 ... x_N}$. Then the likelihood function is:</p>\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "p(D|\\mu) &= \\prod_{n=1}^N \\prod_{k=1}^K \\mu_{k}^{x_{nk}} \\\\\n",
    "         &= \\prod_{k=1}^K \\mu_{k}^{(\\sum_nx_{nk})} \\\\\n",
    "         &= \\prod_{k=1}^K \\mu_k^{m_k}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $m_k = \\sum_n x_{nk}$ which is the number of observations of $x_k=1$\n",
    "\n",
    "We find the maximum likelihood for $\\mu$ by maximizing $\\ln p(D|\\mu)$ and get:\n",
    "\n",
    "$$ \\mu_k^{ML}=\\frac{m_k}{N} $$\n",
    "\n",
    "The <b>multinomial</b> distribution is :\n",
    "\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "Mult(m_1, m_2, ..., m_K|\\mu, N) = \\frac{N!}{m_1!m_2!...m_K!}\\prod_{k=1}^K \\mu_k^{m_k} \n",
    "\\label{eq:mult_dist} \\tag{4}\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dirichlet distribution</h2>\n",
    "<p>The conjugate prior for the multinomial distribution is Dirichlet distribution:</p>\n",
    "\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "Dir(\\mu|\\alpha) = \\frac{\\Gamma(\\alpha_1 +...+ \\alpha_K)}{\\Gamma(\\alpha_1)...\\Gamma(\\alpha_K)}\\prod_{k=1}^K \\mu_k^{\\alpha_k-1}\n",
    "\\label{eq:dir_dist} \\tag{5}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "We can get the posterior distribution for $\\mu$ by multiplying the likelihood function (4) with the prior (5). After normalization we get:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "p(\\mu|D,\\alpha) &= Dir(\\mu|\\alpha+m) \\\\\n",
    "                &= \\frac{\\Gamma(\\alpha_1 +...+ \\alpha_K + N)}{\\Gamma(\\alpha_1+m_1)...\\Gamma(\\alpha_K+m_K)}\\prod_{k=1}^K \\mu_k^{\\alpha_k+m_k-1}\n",
    "\\end{aligned}\n",
    "$$ where $\\alpha_k $ is the number of observations of $x_k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Continuous variables</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gaussian distribution</h2>\n",
    "\n",
    "$$ N(x|\\mu, \\Sigma) = \\frac{1}{(2\\pi^{\\frac{D}{2}})} \\frac{1}{|\\Sigma|^\\frac{1}{2}} e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$$\n",
    "\n",
    "where $\\mu$ is a D dimensioanl vector and $\\Sigma$ is a DxD covariance matrix given by $ \\Sigma_{i,j} = \\sigma(x_i,y_j) $\n",
    "\n",
    "$$ E[x]=\\mu $$\n",
    "\n",
    "$$ cov[x]=\\Sigma $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8949e0270ed4292ba6990c9eef354cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=10.0, description='cov_xx', max=20.0, min=1.0, step=0.5), FloatSlider(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def gaussian_distribution(cov_xx=(1,20,0.5), cov_yy=(1,20,0.5)):\n",
    "    mu_x = 0\n",
    "    variance_x = 3\n",
    "\n",
    "    mu_y = 0\n",
    "    variance_y = 15\n",
    "\n",
    "    x = np.linspace(-10,10,500)\n",
    "    y = np.linspace(-10,10,500)\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "\n",
    "    pos = np.array([X.flatten(),Y.flatten()]).T\n",
    "\n",
    "    rv = multivariate_normal([mu_x, mu_y], [[cov_xx, 0], [0, cov_yy]])\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax0 = fig.add_subplot(111)\n",
    "    ax0.contour(rv.pdf(pos).reshape(500,500))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Partitioned Gaussians</h3>\n",
    "\n",
    "<img src=\"images/1.png\">\n",
    "\n",
    "Given a joint Gaussian distribution $N(x|\\mu, \\Sigma)$ with $\\Lambda \\equiv \\Sigma^{-1}$ and \n",
    "$\n",
    "x= \\begin{pmatrix}\n",
    "    x_a \\\\\n",
    "    x_b \n",
    "    \\end{pmatrix}\n",
    "$ ,\n",
    "$\n",
    "\\mu= \\begin{pmatrix}\n",
    "     \\mu_a \\\\\n",
    "     \\mu_b \n",
    "     \\end{pmatrix}\n",
    "$,\n",
    "$\n",
    "\\Sigma = \\begin{pmatrix}\n",
    "         \\Sigma_{aa} & \\Sigma_{ab} \\\\\n",
    "         \\Sigma_{ba} & \\Sigma_{bb}\n",
    "         \\end{pmatrix}\n",
    "$,\n",
    "$\n",
    "\\Lambda = \\begin{pmatrix}\n",
    "          \\Lambda{aa} & \\Lambda{ab} \\\\\n",
    "          \\Lambda{ba} & \\Lambda{bb}\n",
    "          \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Conditional distribution:\n",
    "\n",
    "$$ p(x_a|x_b) = N(x|\\mu_{a|b}, \\Lambda_{aa}^{-1}) $$\n",
    "$$ \\mu_{a|b} = \\mu_a - \\Lambda^{-1}_{aa} \\Lambda_{ab}(x_b-\\mu_b) $$\n",
    "\n",
    "Marginal distribution:\n",
    "\n",
    "$$ p(x_a) = N(x_a|\\mu_a, \\Sigma_{aa}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bayes' theorem for Gaussian variables</h3>\n",
    "\n",
    "We are given a Gaussian marginal distribution p(x) and a Gaussian conditional distribution p(y|x) which has a mean that is a linear function of x anf covariance independent of x(as above). We want to find the marginal distribution p(y)and conditional distribution p(y|x). We have:\n",
    "\n",
    "$$ p(x) = N(x|\\mu,\\Lambda^{-1}) $$\n",
    "$$ p(y|x) = N(y|Ax+b, L^{-1}) $$\n",
    "\n",
    "where $\\mu$, $A$ and $b$ are parameters governing the means and $\\Lambda$ and $L$ are precisions matrices.\n",
    "\n",
    "we get the following <b>marginal</b> and <b>conditional Gausian</b>:\n",
    "\n",
    "$$ p(y) = N(y|A\\mu+b, L^{-1}+A\\Lambda^{-1}A^T) $$\n",
    "\n",
    "$$ p(x|y) = N(x|\\Sigma{A^TL(y-b) + \\Lambda\\mu}, \\Sigma) $$\n",
    "\n",
    "where $$\\Sigma = (\\Lambda + A^TLA)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Maximum likelihood for the Gaussians</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data set $ X={x_1, ... x_N} $ where each observation is drawn independently from a multivariate Gaussian we can estimate the parameters of the distribution by ML. The derivative of the log likelihood with respect to $ \\mu $ is :\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial\\mu} \\ln p(X|\\mu,\\Sigma) = \\sum_{n=1}^{N}\\Sigma^{-1}(x_n-\\mu) $$\n",
    "\n",
    "Setting this to 0 gives:\n",
    "\n",
    "$$ \\mu_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}x_n $$\n",
    "\n",
    "We can get the maximization for $\\Sigma$ and get:\n",
    "\n",
    "$$ \\Sigma_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}(x_n-\\mu_{ML})(x_n-\\mu_{ML})^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bayesian inference for the Gaussian</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bayesian approsch involves prior distributions.\n",
    "<p></p>\n",
    "<b>Case I - $\\sigma$ is known and infer $\\mu$</b>\n",
    "We have N observations $X={x_1, ... x_N}$, then the likelihood function is:\n",
    "\n",
    "$$ p(x|\\mu) = \\prod_{n=1}^N p(x_n|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{N/2}}e^{{-\\frac{1}{2\\sigma^2}\\sum_{n=1}^{N}(x_n-\\mu)^2}} $$\n",
    "\n",
    "The conjugate prior for this function is a Gaussian so we take the prior function to be:\n",
    "\n",
    "$$ p(\\mu) = N(\\mu|\\mu_0, \\sigma_0^2) $$\n",
    "\n",
    "The posterior distribution then becomes\n",
    "\n",
    "$$ p(\\mu|x) \\propto p(x|\\mu)p(\\mu) $$\n",
    "\n",
    "$$ p(\\mu|x) = N(\\mu|\\mu_N,\\sigma_N^2) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\mu_N = \\frac{\\sigma^2}{N\\sigma_0^2 + \\sigma^2}\\mu_0 + \\frac{N\\sigma_0^2}{N\\sigma_0^2 + \\sigma^2}\\mu_{ML} $$\n",
    "\n",
    "$$ \\frac{1}{\\sigma_N^2} = \\frac{1}{\\sigma_0^2 + \\frac{N}{\\sigma^2}} $$\n",
    "\n",
    "$$ \\mu_{ML} = \\frac{1}{N}\\sum_{n=1}^{N}x_n $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ba37789987444287e1a61a8738ad46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mu', max=2.0, min=-2.0), FloatSlider(value=0.1, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definition of the prior distribution p(\\mu)\n",
    "@interact\n",
    "def prior_gaussian(mu=(-2,2,0.1), variance=(0.1,0.1)):\n",
    "    sigma = math.sqrt(variance)\n",
    "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([0, 5])\n",
    "    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generetae new data from a Gaussian witm mean 0.8 and variance 0.1\n",
    "def generate_gaussian(N):\n",
    "    mu, sigma = 0.8, 0.1\n",
    "    samples = np.random.normal(mu, sigma, N)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f3faac764f4c45b976f8c0b2094458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='N', options=(1, 2, 10), value=1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compute the posterior distribution p(\\mu, x), after observing N data examples\n",
    "@interact\n",
    "def posterior_gaussian(N=[1,2,10]):\n",
    "    var = 0.1\n",
    "    var0 = 0.1\n",
    "    sig = math.sqrt(var)\n",
    "    sig0 = math.sqrt(var0)\n",
    "    mu0=0\n",
    "    muML= np.mean(generate_gaussian(N))\n",
    "    muN = (var/(N*var0 + var))*mu0 + (N*var0)/(N*var0 + var)*muML\n",
    "    sigmaN = pow((1/var0 + N/var),-1)\n",
    "    print(sigmaN)\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([0, 5])\n",
    "    plt.plot(x, stats.norm.pdf(x, muN, math.sqrt(sigmaN)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Mixtures of Gaussians</h3>\n",
    "\n",
    "<img src=\"images/2.png\">\n",
    "\n",
    "Linear combinations of basic distributions such as Gaussians are called <b>mixture distributions</b>. A <b>mixture of Gaussians</b> with K components has the form:\n",
    "\n",
    "$$ p(x) = \\sum_{k=1}^K \\pi_k N(x|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Parameters $\\pi_k$ are called mixing coefficients and \n",
    "\n",
    "$$ \\sum_{k=1}^K \\pi_k = 1 $$\n",
    "\n",
    "<img src=\"images/3.png\">\n",
    "\n",
    "The mixing coefficients satisfy the requirements to be probabilities. The marginal density can be stated as:\n",
    "\n",
    "$$ p(x) = \\sum_{k=1}^K p(k)p(x|k) $$ which is exactly the formula above if we consider $ \\pi_k = p(k) $ and $ N(x|\\mu_k, \\Sigma_k) = p(x|k) $\n",
    "\n",
    "The posterior probabilities $ p(k|x) $ are known as responsabilities and are given by:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\gamma_k(x) &= p(k|x) \\\\\n",
    "            &= \\frac{p(k)p(x|k)}{\\sum_l p(l)p(x|l)} \\\\\n",
    "            &= \\frac{\\pi_k N(x|\\mu_k, \\Sigma_k)}{\\sum_l \\pi_l N(x|\\mu_l, \\sum_l)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Exponential family</h2>\n",
    "\n",
    "The exponential family of distributions over x, given parameters $\\eta$ is defined as:\n",
    "\n",
    "$$ p(x|\\eta) = h(x)g(\\eta)exp\\{\\eta^Tu(x)\\} $$\n",
    "\n",
    "where $\\eta$ is called the natural parameter, u(x) is a function of x and g($\\eta$) ensures the distribution is normalized.\n",
    "\n",
    "Let's consider the <b>Bernoulli distribution</b>:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "p(x|\\mu) &= Bern(x|\\mu) \\\\\n",
    "         &= \\mu^x(1-\\mu)^{1-x} \\\\\n",
    "         &= e^{x\\ln\\mu + (1-x)\\ln(1-\\mu)} \\\\\n",
    "         &= (1-\\mu)e^{\\ln(\\frac{\\mu}{1-\\mu})x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This, we can identify:\n",
    "\n",
    "$$ \\eta = \\ln(\\frac{\\mu}{1-\\mu}) $$ and having $\\mu = \\sigma(\\eta)$ we get\n",
    "\n",
    "$$ \\sigma(\\eta) = \\frac{1}{1+e^{-\\eta}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Nonparametric methods</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>parametric</b> approach tries to determine the parameters of the probability distributions using a data set. The <b>nonparametric</b> approach makes few assumtions about the form of distribution.\n",
    "    \n",
    "<p></p>\n",
    "The histograms methods for density estimation simply partition x into dinstinct bins of width $\\Delta_i$ and then count the number $n_i$ of observations of x falling in bin i. To turn it into a normalized probability density we get:\n",
    "\n",
    "$$ p_i = \\frac{n_i}{N\\Delta_i} $$\n",
    "\n",
    "<img src=\"images/4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Kernel density estimators</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us suppose that observations are being drawn from some unknown probability density p(x) in some D-dimensional space and we want to estmate p(x). Let us consider some small region R containing x. The probability mass associated with this region is given by:\n",
    "\n",
    "$$ P = \\int_{R}p(x)dx $$\n",
    "\n",
    "Now suppose that we have collected a data set comprising N observations drawn from p(x). Because each data point has a probability P of falling within R, the total number K of points that lie inside R will be distributed according to the binomial distribution:\n",
    "\n",
    "$$ Bin(K|N,P) = \\frac{N!}{K!(N-K)!}P^K(1-P)^{1-K} $$\n",
    "\n",
    "We have $E[K/N] = P$ and for large N\n",
    "\n",
    "$$ K \\approx NP $$\n",
    "\n",
    "We also assume that the region R is sufficiently small that the probability density p(x) is roughly constant over the region, then we have:\n",
    "\n",
    "$$ P \\approx p(x)V $$\n",
    "\n",
    "Combining we get:\n",
    "\n",
    "$$ p(x) = \\frac{K}{NV} $$\n",
    "\n",
    "If we fix K and determine V, we have K-nerest-neighboor, if we fix V and determine K we get the kernel approach. In this approach we takethe region R to be a small hypercube centred on the point x at which we wish to determine the probability density. In order to count the number K of points falling within this region, it is convenient to define the following function:\n",
    "\n",
    "$$ \n",
    "k(u) = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        1, & |u_i| <= 1/2, i=1...D \\\\\n",
    "        0 & otherwise\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "which represents a unit cube centred on the origin. The function k(u) is an example of a kernel function. The quantity $ k((x-x_n)/h) $ will be one if the data point $x_n$ lies inside a cube of side h centred on x, and zero otherwise. The total number of data points lying inside this cube will therefore be: \n",
    "\n",
    "$$ K = \\sum_{n=1}^N k(\\frac{x-x_n}{h}) $$\n",
    "\n",
    "Substituing we get:\n",
    "\n",
    "$$ p(x) = \\frac{1}{N}\\sum_{n=1}^N \\frac{1}{h^D}k(\\frac{x-x_n}{h}) $$\n",
    "where $V=h^D$\n",
    "\n",
    "We can interpret this as the sum over N cubes centred on the N data points $x_n$.\n",
    "\n",
    "We can obtain a smoother density model if we choose a smoother kernel function, and a common choice is the Gaussian\n",
    "\n",
    "$$ p(x) = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{(2\\pi h^2)^{1/2}} e^{-\\frac{||x-x_n||^2}{2h^2}} $$\n",
    "\n",
    "where h represents the standard deviation of the Gaussian components.\n",
    "\n",
    "<img src=\"images/5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Nearest-neighbour methods</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a fixed value of K and use the data to find an appropriate value for V. To do this, we consider a small sphere centred on the point x at which we wish to estimate the density p(x), and we allow the radius of the sphere to grow until it contains precisely K data points.\n",
    "\n",
    "<img src=\"images/6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
